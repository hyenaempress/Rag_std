import os
import re
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import docx

class SimpleRAGEngine:
    def __init__(self):
        self.documents = []  # ë©”ëª¨ë¦¬ì— ë¬¸ì„œ ì €ì¥
        print("ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ RAG ì—”ì§„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def add_text_document(self, text, title="ë¬¸ì„œ"):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,  # ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
            chunk_overlap=150,
            separators=["\n\n", "\n", ". ", "! ", "? ", " "]
        )
        
        doc = Document(page_content=text, metadata={"source": title})
        chunks = text_splitter.split_documents([doc])
        self.documents.extend(chunks)
        
        return len(chunks)
    
    def add_file_document(self, file_path):
        """íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¬¸ì„œë¡œ ì¶”ê°€"""
        try:
            if file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
                documents = loader.load()
            elif file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
                documents = loader.load()
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                text = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                documents = [Document(page_content=text, metadata={"source": file_path})]
            else:
                return False, "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=800,
                chunk_overlap=150,
                separators=["\n\n", "\n", ". ", "! ", "? ", " "]
            )
            chunks = text_splitter.split_documents(documents)
            self.documents.extend(chunks)
            
            return True, f"{len(chunks)}ê°œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return False, f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def search_documents(self, query, k=3):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        if not self.documents:
            return []
        
        query_words = self._extract_keywords(query.lower())
        scored_docs = []
        
        for doc in self.documents:
            content = doc.page_content.lower()
            score = self._calculate_relevance_score(content, query_words, query.lower())
            
            if score > 0:
                scored_docs.append((doc, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored_docs[:k]]
    
    def _extract_keywords(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ë¡œ',
            'ì€', 'ëŠ”', 'ì´ë‹¤', 'ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤',
            'ë­', 'ë­”', 'ë­ì•¼', 'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë””', 'ì–¸ì œ', 'ì™œ', 'ì–´ë–»ê²Œ',
            'what', 'is', 'are', 'the', 'a', 'an', 'and', 'or', 'but'
        }
        
        words = query.split()
        meaningful_words = []
        
        for word in words:
            # 2ê¸€ì ì´ìƒì´ê³  ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ê²½ìš°
            if len(word) > 1 and word not in stop_words:
                meaningful_words.append(word)
        
        return meaningful_words
    
    def _calculate_relevance_score(self, content, keywords, full_query):
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)"""
        score = 0
        
        # 1. ì™„ì „í•œ êµ¬ë¬¸ ì¼ì¹˜ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
        if full_query in content:
            score += len(full_query) * 3
        
        # 2. ê°œë³„ í‚¤ì›Œë“œ ì ìˆ˜
        for keyword in keywords:
            if keyword in content:
                # í‚¤ì›Œë“œ ë¹ˆë„ * í‚¤ì›Œë“œ ê¸¸ì´
                frequency = content.count(keyword)
                keyword_score = frequency * len(keyword) * 2
                score += keyword_score
        
        # 3. í‚¤ì›Œë“œ ê·¼ì ‘ì„± ë³´ë„ˆìŠ¤ (í‚¤ì›Œë“œë“¤ì´ ê°€ê¹Œì´ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜)
        if len(keywords) > 1:
            proximity_bonus = self._calculate_proximity_bonus(content, keywords)
            score += proximity_bonus
        
        return score
    
    def _calculate_proximity_bonus(self, content, keywords):
        """í‚¤ì›Œë“œ ê°„ ê·¼ì ‘ì„± ê³„ì‚°"""
        try:
            positions = {}
            for keyword in keywords:
                positions[keyword] = [m.start() for m in re.finditer(keyword, content)]
            
            # ëª¨ë“  í‚¤ì›Œë“œê°€ 100ì ì´ë‚´ì— ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
            for pos_list in positions.values():
                for pos in pos_list:
                    nearby_keywords = 0
                    for other_keyword, other_positions in positions.items():
                        for other_pos in other_positions:
                            if abs(pos - other_pos) <= 100:  # 100ì ì´ë‚´
                                nearby_keywords += 1
                    
                    if nearby_keywords >= len(keywords):
                        return 50  # ê·¼ì ‘ì„± ë³´ë„ˆìŠ¤
            
            return 0
        except:
            return 0
    
    def get_rag_response(self, query):
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ëŒ€í­ ê°œì„ )"""
        relevant_docs = self.search_documents(query, k=3)
        
        if not relevant_docs:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì‘ë‹µ ìƒì„±
        return self._generate_formatted_response(query, relevant_docs)
    
    def _generate_formatted_response(self, query, docs):
        """í¬ë§·ëœ ì‘ë‹µ ìƒì„±"""
        try:
            response_parts = [f'ğŸ’¡ **"{query}"**ì— ëŒ€í•œ ë‹µë³€:\n']
            
            # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            main_content = docs[0].page_content
            
            # 1. í•µì‹¬ ìš”ì•½ (ì²« ë²ˆì§¸ ë¬¸ì„œì—ì„œ)
            summary = self._extract_key_summary(main_content, query)
            if summary:
                response_parts.append(f"**ğŸ“‹ ìš”ì•½:**")
                response_parts.append(f"{summary}\n")
            
            # 2. ìƒì„¸ ì •ë³´ë“¤ (ì—¬ëŸ¬ ë¬¸ì„œì—ì„œ)
            response_parts.append("**ğŸ“š ìƒì„¸ ë‚´ìš©:**")
            
            for i, doc in enumerate(docs[:2], 1):  # ìƒìœ„ 2ê°œ ë¬¸ì„œë§Œ
                source = doc.metadata.get('source', 'ë¬¸ì„œ')
                clean_content = self._clean_and_format_content(doc.page_content)
                
                # 300ìë¡œ ì œí•œí•˜ê³  ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
                truncated = self._smart_truncate(clean_content, 300)
                
                response_parts.append(f"\n**[{i}] ì¶œì²˜: {source}**")
                response_parts.append(f"{truncated}")
            
            # 3. ì¶”ê°€ ë„ì›€ë§
            response_parts.append("\nğŸ’¡ **ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸**ì„ í•˜ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ ìˆ˜ ìˆì–´ìš”!")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ì‘ë‹µ
            return f"ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•˜ì§€ë§Œ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
    
    def _extract_key_summary(self, content, query):
        """í•µì‹¬ ìš”ì•½ ì¶”ì¶œ"""
        try:
            # ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì°¾ê¸°
            sentences = self._split_into_sentences(content)
            query_words = self._extract_keywords(query.lower())
            
            relevant_sentences = []
            for sentence in sentences:
                sentence_lower = sentence.lower()
                if any(word in sentence_lower for word in query_words):
                    relevant_sentences.append(sentence.strip())
                    if len(relevant_sentences) >= 2:  # ìµœëŒ€ 2ë¬¸ì¥
                        break
            
            if relevant_sentences:
                return " ".join(relevant_sentences)
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ì¥ ë°˜í™˜
            return sentences[0].strip() if sentences else None
            
        except:
            return None
    
    def _split_into_sentences(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        
        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
        valid_sentences = []
        for sentence in sentences:
            if len(sentence.strip()) > 10:  # 10ì ì´ìƒì¸ ë¬¸ì¥ë§Œ
                valid_sentences.append(sentence.strip())
        
        return valid_sentences
    
    def _clean_and_format_content(self, content):
        """ë‚´ìš© ì •ë¦¬ ë° í¬ë§·íŒ…"""
        # ì—°ì†ëœ ê³µë°± ì œê±°
        content = re.sub(r'\s+', ' ', content)
        
        # íŠ¹ìˆ˜ë¬¸ìë‚˜ ì´ìƒí•œ ë¬¸ì ì •ë¦¬
        content = re.sub(r'[^\w\sê°€-í£,.!?():\-]', '', content)
        
        # ì•ë’¤ ê³µë°± ì œê±°
        return content.strip()
    
    def _smart_truncate(self, text, max_length):
        """ìŠ¤ë§ˆíŠ¸í•œ í…ìŠ¤íŠ¸ ìë¥´ê¸° (ë¬¸ì¥ ë‹¨ìœ„ë¡œ)"""
        if len(text) <= max_length:
            return text
        
        # max_length ê·¼ì²˜ì—ì„œ ë¬¸ì¥ ëì„ ì°¾ì•„ ìë¥´ê¸°
        truncated = text[:max_length]
        
        # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ ì°¾ê¸°
        last_period = truncated.rfind('.')
        last_exclamation = truncated.rfind('!')
        last_question = truncated.rfind('?')
        
        last_sentence_end = max(last_period, last_exclamation, last_question)
        
        if last_sentence_end > max_length * 0.7:  # 70% ì´ìƒì´ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            return truncated[:last_sentence_end + 1]
        else:
            # ë¬¸ì¥ ëì´ ë„ˆë¬´ ì•ì— ìˆìœ¼ë©´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            last_space = truncated.rfind(' ')
            if last_space > 0:
                return truncated[:last_space] + "..."
            else:
                return truncated + "..."
    
    def get_document_count(self):
        """ì €ì¥ëœ ë¬¸ì„œ ì²­í¬ ê°œìˆ˜"""
        return len(self.documents)
    
    def get_document_stats(self):
        """ë¬¸ì„œ í†µê³„ ì •ë³´"""
        if not self.documents:
            return {"ì´ ë¬¸ì„œ": 0, "í‰ê·  ê¸¸ì´": 0}
        
        sources = {}
        total_length = 0
        
        for doc in self.documents:
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            sources[source] = sources.get(source, 0) + 1
            total_length += len(doc.page_content)
        
        return {
            "ì´ ì²­í¬": len(self.documents),
            "ë¬¸ì„œ ì¢…ë¥˜": len(sources),
            "í‰ê·  ì²­í¬ ê¸¸ì´": total_length // len(self.documents),
            "ë¬¸ì„œë³„ ì²­í¬": sources
        }

# ì „ì—­ RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
rag_engine = SimpleRAGEngine()