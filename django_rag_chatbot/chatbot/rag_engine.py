import os
import re
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import docx

try:
    from pykospacing import spacing
    PYKOSPACING_AVAILABLE = True
except ImportError:
    PYKOSPACING_AVAILABLE = False
    print("PyKoSpacing ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. ê¸°ë³¸ ë„ì–´ì“°ê¸° ë³µì› ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    KONLPY_AVAILABLE = False

class SimpleRAGEngine:
    def __init__(self):
        self.documents = []  # ë©”ëª¨ë¦¬ì— ë¬¸ì„œ ì €ì¥
        print("ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ RAG ì—”ì§„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def add_text_document(self, text, title="ë¬¸ì„œ"):
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # ë” ì‘ì€ ì²­í¬ë¡œ ì„¸ë°€í•˜ê²Œ ë¶„í• 
            chunk_overlap=200,  # ì˜¤ë²„ë© ì¦ê°€ë¡œ ë‚´ìš© ëˆ„ë½ ë°©ì§€
            separators=["\n\n", "\n", ". ", "! ", "? ", "ã€‚", "ï¼Œ", " "]
        )
        
        doc = Document(page_content=text, metadata={"source": title})
        chunks = text_splitter.split_documents([doc])
        self.documents.extend(chunks)
        
        return len(chunks)
    
    def add_file_document(self, file_path):
        """íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¬¸ì„œë¡œ ì¶”ê°€"""
        try:
            if file_path.endswith('.pdf'):
                loader = PyPDFLoader(file_path)
                documents = loader.load()
            elif file_path.endswith('.txt'):
                loader = TextLoader(file_path, encoding='utf-8')
                documents = loader.load()
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                text = '\n'.join([paragraph.text for paragraph in doc.paragraphs])
                documents = [Document(page_content=text, metadata={"source": file_path})]
            else:
                return False, "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤."
            
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=500,  # ë” ì‘ì€ ì²­í¬ë¡œ ì„¸ë°€í•˜ê²Œ ë¶„í• 
                chunk_overlap=200,  # ì˜¤ë²„ë© ì¦ê°€ë¡œ ë‚´ìš© ëˆ„ë½ ë°©ì§€
                separators=["\n\n", "\n", ". ", "! ", "? ", "ã€‚", "ï¼Œ", " "]
            )
            chunks = text_splitter.split_documents(documents)
            self.documents.extend(chunks)
            
            return True, f"{len(chunks)}ê°œ ì²­í¬ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return False, f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def search_documents(self, query, k=3):
        """í‚¤ì›Œë“œ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰ (ê°œì„ ëœ ë²„ì „)"""
        if not self.documents:
            return []
        
        query_words = self._extract_keywords(query.lower())
        print(f"[DEBUG] ê²€ìƒ‰ í‚¤ì›Œë“œ: {query_words}")
        scored_docs = []
        
        for doc in self.documents:
            content = doc.page_content.lower()
            score = self._calculate_relevance_score(content, query_words, query.lower())
            
            if score > 0:
                scored_docs.append((doc, score))
        
        # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ kê°œ ë°˜í™˜
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        
        # ë””ë²„ê·¸: ìƒìœ„ 3ê°œ ê²°ê³¼ ì¶œë ¥
        if scored_docs:
            print(f"[DEBUG] ìƒìœ„ 3ê°œ ê²€ìƒ‰ ê²°ê³¼:")
            for i, (doc, score) in enumerate(scored_docs[:3]):
                preview = doc.page_content[:100].replace('\n', ' ')
                print(f"  {i+1}. ì ìˆ˜: {score}, ë‚´ìš©: {preview}...")
        
        return [doc for doc, score in scored_docs[:k]]
    
    def _extract_keywords(self, query):
        """ì¿¼ë¦¬ì—ì„œ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        # ë¶ˆìš©ì–´ ì œê±° (ì§ˆë¬¸ ë‹¨ì–´ëŠ” ì œì™¸)
        stop_words = {
            'ì´', 'ê·¸', 'ì €', 'ì˜', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì™€', 'ê³¼', 'ìœ¼ë¡œ', 'ë¡œ',
            'ì€', 'ëŠ”', 'ì´ë‹¤', 'ë‹¤', 'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤',
            'ì§€', 'ê¹Œ', 'ë‹ˆ', 'ëƒ', 'ì•¼', 'ì–´', 'ì•„', 'ìš”', 'ì—ìš”',
            'what', 'is', 'are', 'the', 'a', 'an', 'and', 'or', 'but'
        }
        
        # ì§ˆë¬¸ ë‹¨ì–´ë“¤ (ì˜ë¯¸ê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ)
        question_words = {'ë­', 'ë­”', 'ë­ì•¼', 'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë””', 'ì–¸ì œ', 'ì™œ', 'ì–´ë–»ê²Œ'}
        
        words = query.split()
        meaningful_words = []
        
        for word in words:
            # ì •ë¦¬ëœ ë‹¨ì–´ ìƒì„±
            clean_word = word.rstrip('ì§€?!.,').lower()
            
            # 3ê¸€ì ì´ìƒì´ë©´ í•­ìƒ í¬í•¨ (í•µì‹¬ í‚¤ì›Œë“œì¼ ê°€ëŠ¥ì„± ë†’ìŒ)
            if len(clean_word) >= 3:
                meaningful_words.append(clean_word)
            # 2ê¸€ìì´ê³  ë¶ˆìš©ì–´ê°€ ì•„ë‹Œ ê²½ìš°
            elif len(clean_word) == 2 and clean_word not in stop_words:
                meaningful_words.append(clean_word)
            # ì§ˆë¬¸ ë‹¨ì–´ì¸ ê²½ìš° (ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ì˜ë¯¸ê°€ ìˆì„ ìˆ˜ ìˆìŒ)
            elif clean_word in question_words and len(meaningful_words) == 0:
                meaningful_words.append(clean_word)
        
        # ìµœì†Œ í•˜ë‚˜ì˜ í‚¤ì›Œë“œëŠ” ìˆì–´ì•¼ í•¨
        if not meaningful_words and words:
            # ê°€ì¥ ê¸´ ë‹¨ì–´ë¥¼ ì„ íƒ
            longest_word = max(words, key=len).rstrip('ì§€?!.,').lower()
            if len(longest_word) > 0:
                meaningful_words.append(longest_word)
        
        return meaningful_words
    
    def _calculate_relevance_score(self, content, keywords, full_query):
        """ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)"""
        score = 0
        
        # 1. ì™„ì „í•œ êµ¬ë¬¸ ì¼ì¹˜ (ê°€ì¥ ë†’ì€ ì ìˆ˜)
        if full_query in content:
            score += len(full_query) * 5
        
        # 2. ê°œë³„ í‚¤ì›Œë“œ ì ìˆ˜ (í‚¤ì›Œë“œ ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ì ìš©)
        for keyword in keywords:
            if keyword in content:
                # í‚¤ì›Œë“œ ë¹ˆë„
                frequency = content.count(keyword)
                
                # í‚¤ì›Œë“œ ê¸¸ì´ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ (ê¸´ í‚¤ì›Œë“œê°€ ë” ì¤‘ìš”)
                length_weight = len(keyword)
                
                # í•µì‹¬ í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ (5ê¸€ì ì´ìƒì€ ë§¤ìš° ì¤‘ìš”í•œ í‚¤ì›Œë“œë¡œ ê°„ì£¼)
                if len(keyword) >= 5:
                    keyword_score = frequency * length_weight * 10  # ë†’ì€ ê°€ì¤‘ì¹˜
                elif len(keyword) >= 3:
                    keyword_score = frequency * length_weight * 3
                else:
                    keyword_score = frequency * length_weight * 1
                    
                score += keyword_score
                
                # í‚¤ì›Œë“œê°€ ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ì— ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜
                if content[:50].count(keyword) > 0:
                    score += length_weight * 5
        
        # 3. í‚¤ì›Œë“œ ê·¼ì ‘ì„± ë³´ë„ˆìŠ¤ (í‚¤ì›Œë“œë“¤ì´ ê°€ê¹Œì´ ìˆìœ¼ë©´ ì¶”ê°€ ì ìˆ˜)
        if len(keywords) > 1:
            proximity_bonus = self._calculate_proximity_bonus(content, keywords)
            score += proximity_bonus
        
        return score
    
    def _calculate_proximity_bonus(self, content, keywords):
        """í‚¤ì›Œë“œ ê°„ ê·¼ì ‘ì„± ê³„ì‚°"""
        try:
            positions = {}
            for keyword in keywords:
                positions[keyword] = [m.start() for m in re.finditer(keyword, content)]
            
            # ëª¨ë“  í‚¤ì›Œë“œê°€ 100ì ì´ë‚´ì— ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
            for pos_list in positions.values():
                for pos in pos_list:
                    nearby_keywords = 0
                    for other_keyword, other_positions in positions.items():
                        for other_pos in other_positions:
                            if abs(pos - other_pos) <= 100:  # 100ì ì´ë‚´
                                nearby_keywords += 1
                    
                    if nearby_keywords >= len(keywords):
                        return 50  # ê·¼ì ‘ì„± ë³´ë„ˆìŠ¤
            
            return 0
        except:
            return 0
    
    def get_rag_response(self, query):
        """RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± (ì•ˆì „í•œ ë²„ì „)"""
        try:
            relevant_docs = self.search_documents(query, k=3)
            
            if not relevant_docs:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê°„ë‹¨í•˜ê³  ì•ˆì „í•œ ì‘ë‹µ ìƒì„±
            return self._generate_safe_response(query, relevant_docs)
            
        except Exception as e:
            print(f"RAG ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"ê²€ìƒ‰ì€ ì™„ë£Œí–ˆì§€ë§Œ ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}"
    
    def _generate_safe_response(self, query, docs):
        """ì•ˆì „í•œ ì‘ë‹µ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        try:
            response_parts = [f'ğŸ’¡ **"{query}"**ì— ëŒ€í•œ ë‹µë³€:\n']
            
            # ìµœëŒ€ 2ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ì‚¬ìš©
            num_docs_to_use = min(2, len(docs))
            combined_content = []
            sources = []
            
            for i in range(num_docs_to_use):
                doc = docs[i]
                content = doc.page_content
                source = doc.metadata.get('source', 'ë¬¸ì„œ')
                
                # í…ìŠ¤íŠ¸ ì •ë¦¬
                clean_content = self._simple_clean_text(content)
                
                # ê° ë¬¸ì„œì—ì„œ 500ìì”© ê°€ì ¸ì˜¤ê¸°
                if len(clean_content) > 500:
                    clean_content = clean_content[:500]
                
                combined_content.append(clean_content)
                if source not in sources:
                    sources.append(source)
            
            # ì¶œì²˜ í‘œì‹œ
            if sources:
                source_names = [s.split('\\')[-1] if '\\' in s else s for s in sources]
                response_parts.append(f"**ğŸ“š ì¶œì²˜:** {', '.join(source_names[:2])}")
            
            # ë‚´ìš© ê²°í•© (ìµœëŒ€ 800ì)
            full_content = '\n\n'.join(combined_content)
            if len(full_content) > 800:
                full_content = full_content[:800] + "..."
            
            response_parts.append(f"**ë‚´ìš©:**\n{full_content}")
            
            # ì¶”ê°€ ì•ˆë‚´
            if len(docs) > 2:
                response_parts.append(f"\nğŸ“„ ì¶”ê°€ë¡œ {len(docs)-2}ê°œì˜ ê´€ë ¨ ë¬¸ì„œê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            response_parts.append("\nğŸ’¡ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ë‹µë³€ì„ ë“œë¦´ê²Œìš”!")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            print(f"ì•ˆì „í•œ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨ - ì•„ì£¼ ê°„ë‹¨í•œ ì‘ë‹µ
            try:
                content = docs[0].page_content[:200] + "..."
                return f'ğŸ’¡ **"{query}"**ì— ëŒ€í•œ ë‹µë³€:\n\n{content}\n\nğŸ’¡ ë” ìì„¸í•œ ë‚´ìš©ì´ í•„ìš”í•˜ì‹œë©´ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!'
            except:
                return "ë¬¸ì„œë¥¼ ì°¾ì•˜ì§€ë§Œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _simple_clean_text(self, text):
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì •ë¦¬ (ë„ì–´ì“°ê¸° ë³µì› í¬í•¨)"""
        try:
            # ê¸°ë³¸ ì •ë¦¬
            text = re.sub(r'\s+', ' ', text)  # ì—°ì† ê³µë°± ì œê±°
            text = text.strip()
            
            # ê³ ê¸‰ ë„ì–´ì“°ê¸° ë³µì› í•¨ìˆ˜ ì‚¬ìš©
            text = self.restore_korean_spacing(text)
            
            return text
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ë„ì–´ì“°ê¸°ë§Œ ì ìš©
            text = re.sub(r'([.!?])([ê°€-í£A-Za-z])', r'\1 \2', text)
            text = re.sub(r'([A-Za-z])([ê°€-í£])', r'\1 \2', text)
            text = re.sub(r'([ê°€-í£])([A-Za-z])', r'\1 \2', text)
            return text
    
    def _extract_key_summary(self, content, query):
        """í•µì‹¬ ìš”ì•½ ì¶”ì¶œ"""
        try:
            # ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ë“¤ ì°¾ê¸°
            sentences = self._split_into_sentences(content)
            query_words = self._extract_keywords(query.lower())
            
            relevant_sentences = []
            for sentence in sentences:
                sentence_lower = sentence.lower()
                if any(word in sentence_lower for word in query_words):
                    relevant_sentences.append(sentence.strip())
                    if len(relevant_sentences) >= 2:  # ìµœëŒ€ 2ë¬¸ì¥
                        break
            
            if relevant_sentences:
                return " ".join(relevant_sentences)
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ì¥ ë°˜í™˜
            return sentences[0].strip() if sentences else None
            
        except:
            return None
    
    def _split_into_sentences(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• """
        # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', text)
        
        # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œê±°
        valid_sentences = []
        for sentence in sentences:
            if len(sentence.strip()) > 10:  # 10ì ì´ìƒì¸ ë¬¸ì¥ë§Œ
                valid_sentences.append(sentence.strip())
        
        return valid_sentences
    
    def _clean_and_format_content(self, content):
        """ë‚´ìš© ì •ë¦¬ ë° í¬ë§·íŒ… (ë„ì–´ì“°ê¸° ë³µì› í¬í•¨)"""
        # 1. ê¸°ë³¸ ì •ë¦¬
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'[^\w\sê°€-í£,.!?():\-]', '', content)
        content = content.strip()
        
        # 2. ë„ì–´ì“°ê¸° ë³µì› (ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš©)
        content = self.restore_korean_spacing(content)
        
        return content
    
    def restore_korean_spacing(self, text):
        """í•œêµ­ì–´ ë„ì–´ì“°ê¸° ë³µì› í•¨ìˆ˜ - 3ë‹¨ê³„ ì ‘ê·¼ë²•"""
        if not text or len(text.strip()) == 0:
            return text
            
        try:
            # 1ë‹¨ê³„: PyKoSpacing ì‚¬ìš© (ê°€ì¥ ì¢‹ì€ í’ˆì§ˆ)
            if PYKOSPACING_AVAILABLE:
                return spacing(text)
            
            # 2ë‹¨ê³„: KoNLPy ì‚¬ìš© (í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜)
            elif KONLPY_AVAILABLE:
                return self._restore_spacing_with_konlpy(text)
            
            # 3ë‹¨ê³„: íŒ¨í„´ ë§¤ì¹­ ê¸°ë°˜ (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)
            else:
                return self._restore_spacing_with_patterns(text)
                
        except Exception as e:
            print(f"ë„ì–´ì“°ê¸° ë³µì› ì˜¤ë¥˜: {e}")
            return self._restore_spacing_with_patterns(text)
    
    def _restore_spacing_with_konlpy(self, text):
        """KoNLPyë¥¼ ì‚¬ìš©í•œ ë„ì–´ì“°ê¸° ë³µì›"""
        try:
            okt = Okt()
            
            # í˜•íƒœì†Œ ë¶„ì„ í›„ ë„ì–´ì“°ê¸° ì ìš© (normalize, stem íŒŒë¼ë¯¸í„° ì œê±°)
            morphs = okt.morphs(text)
            pos_tags = okt.pos(text)
            
            spaced_text = ""
            for i, (morph, pos) in enumerate(pos_tags):
                if i == 0:
                    spaced_text += morph
                else:
                    # ì¡°ì‚¬, ì–´ë¯¸, ì ‘ë¯¸ì‚¬ëŠ” ë¶™ì—¬ì“°ê³  ë‚˜ë¨¸ì§€ëŠ” ë„ì–´ì“°ê¸°
                    if pos in ['Josa', 'Eomi', 'Suffix']:
                        spaced_text += morph
                    else:
                        spaced_text += " " + morph
            
            return spaced_text
            
        except Exception as e:
            print(f"KoNLPy ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return self._restore_spacing_with_patterns(text)
    
    def _restore_spacing_with_patterns(self, text):
        """íŒ¨í„´ ë§¤ì¹­ì„ ì‚¬ìš©í•œ ë„ì–´ì“°ê¸° ë³µì› (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´)"""
        if not text:
            return text
        
        # ê¸°ë³¸ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        patterns = [
            # ë¬¸ì¥ë¶€í˜¸ ë’¤ì— ë„ì–´ì“°ê¸°
            (r'([.!?])([ê°€-í£A-Za-z0-9])', r'\1 \2'),
            (r'([,;:])([ê°€-í£A-Za-z0-9])', r'\1 \2'),
            
            # ìˆ«ìì™€ ë¬¸ì ì‚¬ì´
            (r'([0-9])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([0-9])', r'\1 \2'),
            
            # ì˜ì–´ì™€ í•œê¸€ ì‚¬ì´
            (r'([A-Za-z])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([A-Za-z])', r'\1 \2'),
            
            # ìì£¼ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ë“¤ ë’¤
            (r'(ì´ë‹¤|í•˜ë‹¤|ë˜ë‹¤|ìˆë‹¤|ì—†ë‹¤|ê°™ë‹¤|ì´ë©°|ë¼ê³ |ì´ë¼ê³ |í•œë‹¤|ëœë‹¤)([ê°€-í£])', r'\1 \2'),
            (r'(ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ë˜í•œ|ë”°ë¼ì„œ|ì¦‰|ì˜ˆë¥¼ë“¤ì–´|ë•Œë¬¸ì—)([ê°€-í£])', r'\1 \2'),
            (r'(ê²½ìš°ì—|ê´€ë ¨í•˜ì—¬|ëŒ€í•˜ì—¬|í†µí•˜ì—¬|ìœ„í•˜ì—¬|ì˜í•˜ì—¬)([ê°€-í£])', r'\1 \2'),
            
            # ê¸°ìˆ ìš©ì–´ì™€ í•œê¸€ ì‚¬ì´
            (r'(AI|ML|LLM|RAG|API|GPU|CPU|NLP|CNN|RNN|IoT|VR|AR)([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])(AI|ML|LLM|RAG|API|GPU|CPU|NLP|CNN|RNN|IoT|VR|AR)', r'\1 \2'),
            
            # ì¡°ì‚¬ ì•ë’¤ ì²˜ë¦¬ (ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ)
            (r'([ê°€-í£])(ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ë¡œì™€ê³¼ë„ë§Œí¼ë¶€í„°ê¹Œì§€ë§ˆë‹¤ì—ê²Œê»˜ì„œ)([ê°€-í£A-Z])', r'\1\2 \3'),
            
        ]
        
        result = text
        for pattern, replacement in patterns:
            result = re.sub(pattern, replacement, result)
        
        # ê¸´ í•œê¸€ ë‹¨ì–´ ë¶„í•  (ë³„ë„ ì²˜ë¦¬)
        result = self._split_long_korean_words(result)
        
        # ìµœì¢… ì •ë¦¬
        result = re.sub(r'\s+', ' ', result)
        result = re.sub(r' +([,.!?;:])', r'\1', result)
        
        return result.strip()
    
    def _split_long_korean_words(self, text):
        """ê¸´ í•œê¸€ ë‹¨ì–´ë“¤ì„ ë¶„í• """
        def split_word(match):
            word = match.group(0)
            if len(word) <= 8:
                return word
                
            # 15ì ì´ìƒì´ë©´ 3ë“±ë¶„
            if len(word) > 15:
                third = len(word) // 3
                return word[:third] + ' ' + word[third:2*third] + ' ' + word[2*third:]
            # 8ì ì´ìƒì´ë©´ ë°˜ìœ¼ë¡œ
            else:
                half = len(word) // 2
                return word[:half] + ' ' + word[half:]
        
        # 8ì ì´ìƒ ì—°ì†ëœ í•œê¸€ ë‹¨ì–´ ì°¾ì•„ì„œ ë¶„í• 
        return re.sub(r'[ê°€-í£]{8,}', split_word, text)
    
    def _smart_split_word(self, word):
        """ê¸´ ë‹¨ì–´ë¥¼ ìŠ¤ë§ˆíŠ¸í•˜ê²Œ ë¶„í•  (í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)"""
        if len(word) <= 8:
            return word
            
        # 15ì ì´ìƒì´ë©´ 3ë“±ë¶„
        if len(word) > 15:
            third = len(word) // 3
            return word[:third] + ' ' + word[third:2*third] + ' ' + word[2*third:]
        # 8ì ì´ìƒì´ë©´ ë°˜ìœ¼ë¡œ
        else:
            half = len(word) // 2
            return word[:half] + ' ' + word[half:]

    def _restore_spacing_advanced(self, text):
        """ê³ ê¸‰ ë„ì–´ì“°ê¸° ë³µì› (ê¸°ì¡´ í•¨ìˆ˜ ìœ ì§€ - í˜¸í™˜ì„±ì„ ìœ„í•´)"""
        return self.restore_korean_spacing(text)
    
    def _restore_spacing_enhanced(self, text):
        """ê°•í™”ëœ ë„ì–´ì“°ê¸° ë³µì› (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—†ì´) - í•œêµ­ì–´ íŠ¹í™”"""
        if not text or len(text) < 2:
            return text
        
        # 1. ê¸°ë³¸ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 2. í•œêµ­ì–´ ë„ì–´ì“°ê¸° íŒ¨í„´ë“¤ (ì‹¤ì „ íŠ¹í™”)
        patterns = [
            # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤
            (r'([.!?])([ê°€-í£A-Za-zê°€-í£])', r'\1 \2'),
            (r'([,;:])([ê°€-í£A-Za-z])', r'\1 \2'),
            
            # ìˆ«ìì™€ í•œê¸€ ì‚¬ì´
            (r'([0-9])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([0-9])', r'\1 \2'),
            
            # ì˜ì–´ì™€ í•œê¸€ ì‚¬ì´  
            (r'([A-Za-z])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([A-Za-z])', r'\1 \2'),
            
            # ì¤‘ìš”: ìì£¼ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ ë’¤ì— ë„ì–´ì“°ê¸°
            (r'(ì´ë‹¤|í•˜ë‹¤|ë˜ë‹¤|ìˆë‹¤|ì—†ë‹¤|ê°™ë‹¤|ì´ë©°|ë¼ê³ |ì´ë¼ê³ )([ê°€-í£])', r'\1 \2'),
            (r'(ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ë˜í•œ|ë”°ë¼ì„œ|ì¦‰|ì˜ˆë¥¼ë“¤ì–´)([ê°€-í£])', r'\1 \2'),
            (r'(ë•Œë¬¸ì—|ê²½ìš°ì—|ê´€ë ¨í•˜ì—¬|ëŒ€í•˜ì—¬|í†µí•˜ì—¬)([ê°€-í£])', r'\1 \2'),
            
            # ê¸°ìˆ ìš©ì–´ ë¶„ë¦¬
            (r'(LLM|RAG|AI|ML|API|GPU|CPU|NLP|CNN|RNN)([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])(LLM|RAG|AI|ML|API|GPU|CPU|NLP|CNN|RNN)', r'\1 \2'),
            
            # íŠ¹ì • íŒ¨í„´ - ì‹¤ì œ ë¬¸ì„œì—ì„œ ìì£¼ ë³´ì´ëŠ” ê²ƒë“¤
            (r'([ê°€-í£])([CDEFGHIJKLMNOPQRSTUVWXYZê°€-í£]{2,})', r'\1 \2'),
            (r'(ì„ìˆ˜|ë¥¼ìˆ˜|ì—ëŒ€í•´|ì—ê´€í•´|ë¡œë¶€í„°)([ê°€-í£])', r'\1 \2'),
            
            # ì¡°ì‚¬ ì•ë’¤ (ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ)
            (r'([ê°€-í£])(ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì˜ë¡œì™€ê³¼ë„ë§Œí¼ë¶€í„°ê¹Œì§€ë§ˆë‹¤)([ê°€-í£A-Z])', r'\1\2 \3'),
        ]
        
        result = text
        for pattern, replacement in patterns:
            result = re.sub(pattern, replacement, result)
        
        # 3. íŠ¹ë³„íˆ ê¸´ ë‹¨ì–´ë“¤ ì²˜ë¦¬ (ë¬¸ì„œì—ì„œ ìì£¼ ë³´ì´ëŠ” íŒ¨í„´)
        result = self._break_long_words(result)
        
        # 4. ì •ë¦¬
        result = re.sub(r'\s+', ' ', result)
        result = re.sub(r' +([,.!?;:])', r'\1', result)
        
        return result.strip()
    
    def _break_long_words(self, text):
        """ê³¼ë„í•˜ê²Œ ê¸´ ì—°ì† ë‹¨ì–´ë“¤ì„ ì ì ˆíˆ ë¶„í• """
        # 10ì ì´ìƒ ì—°ì†ëœ í•œê¸€ì„ ì°¾ì•„ì„œ ì¤‘ê°„ì— ë„ì–´ì“°ê¸° ì¶”ê°€
        def split_long_korean(match):
            word = match.group(0)
            if len(word) > 15:  # 15ì ì´ìƒì´ë©´ 3ë“±ë¶„
                third = len(word) // 3
                return word[:third] + ' ' + word[third:2*third] + ' ' + word[2*third:]
            elif len(word) > 8:  # 8ì ì´ìƒì´ë©´ ë°˜ìœ¼ë¡œ
                half = len(word) // 2
                return word[:half] + ' ' + word[half:]
            return word
        
        # ì—°ì†ëœ í•œê¸€ íŒ¨í„´ ì°¾ê¸°
        text = re.sub(r'[ê°€-í£]{8,}', split_long_korean, text)
        
        return text
    
    def _restore_spacing(self, text):
        """ë„ì–´ì“°ê¸° ë³µì› í•¨ìˆ˜"""
        if not text:
            return text
        
        # í•œêµ­ì–´ ë„ì–´ì“°ê¸° íŒ¨í„´ ì ìš©
        patterns = [
            # ì¡°ì‚¬ ì•ì— ë„ì–´ì“°ê¸°
            (r'([ê°€-í£])([ì€ëŠ”ì´ê°€ì„ë¥¼ì—ì„œì™€ê³¼ë¡œìœ¼ë¡œì˜])([ê°€-í£])', r'\1\2 \3'),
            
            # ë§ˆì¹¨í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œ ë’¤ì— ë„ì–´ì“°ê¸°  
            (r'([.!?])([ê°€-í£A-Za-z])', r'\1 \2'),
            
            # ì‰¼í‘œ ë’¤ì— ë„ì–´ì“°ê¸°
            (r'([,])([ê°€-í£A-Za-z])', r'\1 \2'),
            
            # ìˆ«ìì™€ í•œê¸€ ì‚¬ì´
            (r'([0-9])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([0-9])', r'\1 \2'),
            
            # ì˜ì–´ì™€ í•œê¸€ ì‚¬ì´
            (r'([A-Za-z])([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])([A-Za-z])', r'\1 \2'),
            
            # íŠ¹ì • ë‹¨ì–´ë“¤ ë’¤ì— ë„ì–´ì“°ê¸°
            (r'(ì´ë‹¤|ìˆë‹¤|ì—†ë‹¤|í•˜ë‹¤|ë˜ë‹¤|ê°™ë‹¤)([ê°€-í£])', r'\1 \2'),
            (r'(ê·¸ë¦¬ê³ |í•˜ì§€ë§Œ|ê·¸ëŸ¬ë‚˜|ë”°ë¼ì„œ|ë˜í•œ)([ê°€-í£])', r'\1 \2'),
            
            # ìì£¼ ì‚¬ìš©ë˜ëŠ” ì ‘ì†ì–´ë“¤
            (r'(LLM|RAG|AI|ML)([ê°€-í£])', r'\1 \2'),
            (r'([ê°€-í£])(LLM|RAG|AI|ML)', r'\1 \2'),
        ]
        
        result = text
        for pattern, replacement in patterns:
            result = re.sub(pattern, replacement, result)
        
        # ê³¼ë„í•œ ë„ì–´ì“°ê¸° ì •ë¦¬
        result = re.sub(r'\s+', ' ', result)
        
    def _restore_spacing_advanced(self, text):
        """ê³ ê¸‰ ë„ì–´ì“°ê¸° ë³µì› (ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)"""
        try:
            # python-spacing ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì„¤ì¹˜ëœ ê²½ìš°)
            from spacing import spacing
            return spacing(text)
        except ImportError:
            try:
                # khaiii ì‚¬ìš© (ì„¤ì¹˜ëœ ê²½ìš°)
                from khaiii import KhaiiiApi
                api = KhaiiiApi()
                spaced_text = ""
                
                for word in api.analyze(text):
                    spaced_text += word.lex + " "
                
                return spaced_text.strip()
            except ImportError:
                # ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë°©ë²• ì‚¬ìš©
                return self._restore_spacing(text)
        except Exception:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ ë°©ë²• ì‚¬ìš©
            return self._restore_spacing(text)
    
    def _smart_truncate(self, text, max_length):
        """ìŠ¤ë§ˆíŠ¸í•œ í…ìŠ¤íŠ¸ ìë¥´ê¸° (ë¬¸ì¥ ë‹¨ìœ„ë¡œ)"""
        if len(text) <= max_length:
            return text
        
        # max_length ê·¼ì²˜ì—ì„œ ë¬¸ì¥ ëì„ ì°¾ì•„ ìë¥´ê¸°
        truncated = text[:max_length]
        
        # ë§ˆì§€ë§‰ ì™„ì „í•œ ë¬¸ì¥ ì°¾ê¸°
        last_period = truncated.rfind('.')
        last_exclamation = truncated.rfind('!')
        last_question = truncated.rfind('?')
        
        last_sentence_end = max(last_period, last_exclamation, last_question)
        
        if last_sentence_end > max_length * 0.7:  # 70% ì´ìƒì´ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            return truncated[:last_sentence_end + 1]
        else:
            # ë¬¸ì¥ ëì´ ë„ˆë¬´ ì•ì— ìˆìœ¼ë©´ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìë¥´ê¸°
            last_space = truncated.rfind(' ')
            if last_space > 0:
                return truncated[:last_space] + "..."
            else:
                return truncated + "..."
    
    def get_document_count(self):
        """ì €ì¥ëœ ë¬¸ì„œ ì²­í¬ ê°œìˆ˜"""
        return len(self.documents)
    
    def get_document_stats(self):
        """ë¬¸ì„œ í†µê³„ ì •ë³´"""
        if not self.documents:
            return {"ì´ ë¬¸ì„œ": 0, "í‰ê·  ê¸¸ì´": 0}
        
        sources = {}
        total_length = 0
        
        for doc in self.documents:
            source = doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ')
            sources[source] = sources.get(source, 0) + 1
            total_length += len(doc.page_content)
        
        return {
            "ì´ ì²­í¬": len(self.documents),
            "ë¬¸ì„œ ì¢…ë¥˜": len(sources),
            "í‰ê·  ì²­í¬ ê¸¸ì´": total_length // len(self.documents),
            "ë¬¸ì„œë³„ ì²­í¬": sources
        }

# ì „ì—­ RAG ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
rag_engine = SimpleRAGEngine()